{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **PRÁCTICA FINAL - PREDICCIÓN DE DURACIÓN DE VIAJES Y DETECCIÓN DE EMERGECIAS EN TWEETS**\n",
    "\n",
    "**Asignatura:** Modelos no supervisados\n",
    "\n",
    "**Fecha:** 09/06/2023\n",
    "\n",
    "**Autores:** Mencía de Parias y Juan María Rotaeche"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **PARTE 0: Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PARTE 1: Predicción de duración de viajes**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ejercicio 1**\n",
    "\n",
    "Realizar preprocesamiento de datos: imputar valores faltantes, transformar variables categóricas, estandarizar variables numéricas, etc. \n",
    "si lo consideras necesario para futuros modelos. Puede ser interesante intentar adaptar las variables que no siguan una distribución normal mediante técnicas de mapeado a gausianas como Power Transformers (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html#sklearn.preprocessing.PowerTransformer). *(2puntos)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_examples = pd.read_csv('uber_time_examples.csv')\n",
    "uber_labels = pd.read_csv('uber_time_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 12) (400000, 2)\n",
      "   id       feature_0  feature_1  feature_2  feature_3  feature_4  feature_5  \\\n",
      "0   0  01-07 17:04:08          2       1.20        263        141  12.513054   \n",
      "1   1  03-02 17:41:40          1       0.88        246         68   6.256527   \n",
      "2   2  02-17 12:15:00          3       7.61         24         13  18.769581   \n",
      "3   3  03-30 13:59:42          1       1.50        239        163   6.256527   \n",
      "4   4  02-14 18:26:55          1       1.20        142        229   6.256527   \n",
      "\n",
      "    feature_6  feature_7  feature_8  feature_9  feature_10  \n",
      "0  297.430685  56.317405     405.20   0.408689  126.689773  \n",
      "1  278.205127  27.160167     314.88  -0.256911  126.693467  \n",
      "2   27.141964   5.192385      44.61  56.880789  126.615789  \n",
      "3  270.288721  65.104518     403.50   1.218689  126.686311  \n",
      "4  160.589952  91.465857     372.20   0.408689  126.689773      id  duration\n",
      "0   0     455.0\n",
      "1   1     413.0\n",
      "2   2    1501.0\n",
      "3   3     514.0\n",
      "4   4     605.0\n"
     ]
    }
   ],
   "source": [
    "print (uber_examples.shape, \n",
    "       uber_labels.shape)\n",
    "print (uber_examples.head(), \n",
    "       uber_labels.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambas tablas tienen el mismo número de filas y la columna *id* para identificar los registros, por lo que el primer paso es unir los datos en base a esta columna."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>duration</th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>feature_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>455.0</td>\n",
       "      <td>01-07 17:04:08</td>\n",
       "      <td>2</td>\n",
       "      <td>1.20</td>\n",
       "      <td>263</td>\n",
       "      <td>141</td>\n",
       "      <td>12.513054</td>\n",
       "      <td>297.430685</td>\n",
       "      <td>56.317405</td>\n",
       "      <td>405.20</td>\n",
       "      <td>0.408689</td>\n",
       "      <td>126.689773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>413.0</td>\n",
       "      <td>03-02 17:41:40</td>\n",
       "      <td>1</td>\n",
       "      <td>0.88</td>\n",
       "      <td>246</td>\n",
       "      <td>68</td>\n",
       "      <td>6.256527</td>\n",
       "      <td>278.205127</td>\n",
       "      <td>27.160167</td>\n",
       "      <td>314.88</td>\n",
       "      <td>-0.256911</td>\n",
       "      <td>126.693467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1501.0</td>\n",
       "      <td>02-17 12:15:00</td>\n",
       "      <td>3</td>\n",
       "      <td>7.61</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>18.769581</td>\n",
       "      <td>27.141964</td>\n",
       "      <td>5.192385</td>\n",
       "      <td>44.61</td>\n",
       "      <td>56.880789</td>\n",
       "      <td>126.615789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>514.0</td>\n",
       "      <td>03-30 13:59:42</td>\n",
       "      <td>1</td>\n",
       "      <td>1.50</td>\n",
       "      <td>239</td>\n",
       "      <td>163</td>\n",
       "      <td>6.256527</td>\n",
       "      <td>270.288721</td>\n",
       "      <td>65.104518</td>\n",
       "      <td>403.50</td>\n",
       "      <td>1.218689</td>\n",
       "      <td>126.686311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>605.0</td>\n",
       "      <td>02-14 18:26:55</td>\n",
       "      <td>1</td>\n",
       "      <td>1.20</td>\n",
       "      <td>142</td>\n",
       "      <td>229</td>\n",
       "      <td>6.256527</td>\n",
       "      <td>160.589952</td>\n",
       "      <td>91.465857</td>\n",
       "      <td>372.20</td>\n",
       "      <td>0.408689</td>\n",
       "      <td>126.689773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  duration       feature_0  feature_1  feature_2  feature_3  feature_4  \\\n",
       "0   0     455.0  01-07 17:04:08          2       1.20        263        141   \n",
       "1   1     413.0  03-02 17:41:40          1       0.88        246         68   \n",
       "2   2    1501.0  02-17 12:15:00          3       7.61         24         13   \n",
       "3   3     514.0  03-30 13:59:42          1       1.50        239        163   \n",
       "4   4     605.0  02-14 18:26:55          1       1.20        142        229   \n",
       "\n",
       "   feature_5   feature_6  feature_7  feature_8  feature_9  feature_10  \n",
       "0  12.513054  297.430685  56.317405     405.20   0.408689  126.689773  \n",
       "1   6.256527  278.205127  27.160167     314.88  -0.256911  126.693467  \n",
       "2  18.769581   27.141964   5.192385      44.61  56.880789  126.615789  \n",
       "3   6.256527  270.288721  65.104518     403.50   1.218689  126.686311  \n",
       "4   6.256527  160.589952  91.465857     372.20   0.408689  126.689773  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber = pd.merge (uber_labels, uber_examples, on = 'id')\n",
    "# uber.set_index('id', inplace= True)\n",
    "uber.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos los datos listos para empezar a trabajar, primero vamos a realizar un preprocesamiento de datos. Para ello, primero trabajaremos con los valores nulos si los hubiera; después analizaremos el tipo de variables que tenemos y si es necesario transformar alguna; y si realizamos una transformación a las variables que no siguen una distribución normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            0\n",
       "duration      0\n",
       "feature_0     0\n",
       "feature_1     0\n",
       "feature_2     0\n",
       "feature_3     0\n",
       "feature_4     0\n",
       "feature_5     0\n",
       "feature_6     0\n",
       "feature_7     0\n",
       "feature_8     0\n",
       "feature_9     0\n",
       "feature_10    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En todo nuestro conjunto de datos no hay ningún valor faltante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id              int64\n",
       "duration      float64\n",
       "feature_0      object\n",
       "feature_1       int64\n",
       "feature_2     float64\n",
       "feature_3       int64\n",
       "feature_4       int64\n",
       "feature_5     float64\n",
       "feature_6     float64\n",
       "feature_7     float64\n",
       "feature_8     float64\n",
       "feature_9     float64\n",
       "feature_10    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "uber['feature_0'] = pd.to_datetime(uber['feature_0'], format='%m-%y %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\menci\\anaconda3\\lib\\site-packages\\scipy\\stats\\morestats.py:1760: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La variable 'id' no sigue una distribución normal.\n",
      "La variable 'duration' no sigue una distribución normal.\n",
      "La variable 'feature_0' no sigue una distribución normal.\n",
      "La variable 'feature_1' no sigue una distribución normal.\n",
      "La variable 'feature_2' no sigue una distribución normal.\n",
      "La variable 'feature_3' no sigue una distribución normal.\n",
      "La variable 'feature_4' no sigue una distribución normal.\n",
      "La variable 'feature_5' no sigue una distribución normal.\n",
      "La variable 'feature_6' no sigue una distribución normal.\n",
      "La variable 'feature_7' no sigue una distribución normal.\n",
      "La variable 'feature_8' no sigue una distribución normal.\n",
      "La variable 'feature_9' no sigue una distribución normal.\n",
      "La variable 'feature_10' no sigue una distribución normal.\n"
     ]
    }
   ],
   "source": [
    "for columna in uber.columns:\n",
    "    datos = uber[columna].values\n",
    "    stat, p_valor = stats.shapiro(datos) # Prueba de Shapiro-Wilk\n",
    "    alpha = 0.05 # Comprobar la normalidad\n",
    "    if p_valor > alpha:\n",
    "        print(f\"La variable '{columna}' sigue una distribución normal.\")\n",
    "    else:\n",
    "        print(f\"La variable '{columna}' no sigue una distribución normal.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "columnas_transformar = ['feature_1','feature_2','feature_3','feature_4','feature_5','feature_6','feature_7','feature_8','feature_9','feature_10']\n",
    "\n",
    "transformador = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "uber[columnas_transformar] = transformador.fit_transform(uber[columnas_transformar])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La única columna a la que conviene cambiarle el tipo de datos es a *feature_0* ya que muestra una fecha pero está en formato object.\n",
    "\n",
    "Hemos realizado la prueba de Shapiro-Wilk para comprobar qué variables no siguen una distribución normal, y posteriormente hemos normalizado todas las variables excepto la fecha y el id con el método Yeo-Johnson de PowerTransformer."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ejercicio 2**\n",
    "\n",
    "Crear nuevas características (features) que puedan mejorar el poder predictivo del modelo. *(1 punto)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id            400000\n",
       "duration        5592\n",
       "feature_0     387400\n",
       "feature_1         10\n",
       "feature_2       2709\n",
       "feature_3        231\n",
       "feature_4        255\n",
       "feature_5         10\n",
       "feature_6        231\n",
       "feature_7        255\n",
       "feature_8      39968\n",
       "feature_9       2709\n",
       "feature_10      2709\n",
       "dtype: int64"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uber.nunique() #ver si es conveniente poner feature_1 y feature_5 como dummies"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ejercicio 3**\n",
    "\n",
    "Seleccionar las características más relevantes para predecir la duración del viaje. Utilizar técnicas de selección de características basadas en una sola variable o SelectFromModel. Evitar Recursive feature elimination debido a su alto coste computacional.\n",
    "Ver : (https://scikit-learn.org/stable/modules/feature_selection.html) *(2puntos)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ejercicio 4**\n",
    "\n",
    "Entrenar un modelo sencillo como base y medir su MAPE (Mean Absolute Percentage Error) en el conjunto de test. Luego, elegir y entrenar dos modelos más avanzados (por ejemplo, ensambladores, máquinas de soporte vectorial, modelos bayesianos, redes neuronales) y comparar sus MAPEs. *(2puntos)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ejercicio 5**\n",
    "\n",
    "Optimizar los hiperparámetros de los dos últimos modelos utilizando validación cruzada (cross-validation) y comparar sus MAPEs. Elegir el mejor modelo basándose en estos resultados. *(2puntos)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PARTE 2: Detección de emergencias en tweets**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ejercicio 1:**\n",
    "\n",
    "Extraer los embeddings del texto de los tweets utilizando un modelo pre-entrenado de Huggingface. *(1.5 puntos)*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ejercicio 2:**\n",
    "\n",
    "Crear y entrenar una pequeña red neuronal que utilice los embeddings, la palabra clave (keyword) y la ubicación (location) para predecir si un tweet está relacionado con una emergencia o no. Gestionar los valores faltantes y agrupar las variables categóricas de manera adecuada. No es necesario realizar una optimización de hiperparámetros exhaustiva, pero se pueden realizar ajustes si se desea. \n",
    "\n",
    "NOTA: Si no se ha podido calcular los embeddings del ejercicio anterior, usar los que aparecen guardados como numpy.array en el fichero. *(1.5 puntos)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
